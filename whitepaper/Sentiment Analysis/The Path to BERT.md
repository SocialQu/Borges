# The Path to BERT

**Introduction:** what is so special about BERT.

![Contextual Representation of Words Vectors]()


Index:
* LSTMs
* Sequence to Sequence Learning
* Semi Supervised Sequence Learning
* Deep Contextualized Word Representations.
* ELMO
* GPT-1

***

## Sequence to Sequence Learning

* Problem when making predictions of variable length.
* Auto-encoders and end of tokens.
* 
* 

***

## Semi Supervised Sequence Learning

* The problem with randomized initial word vectors.
* Using unlabeled data to improve results.
* Applications from sentiment analysis to image classification.
* 


*** 
