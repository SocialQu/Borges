# 11. Quiz 


## Multiple Choice 
1. How antonyms are computed?
* [ ] Find the word that minimizes the dot product computation.
* [ ] Find the word that maximizes the dot product computation.
* [ ] Define a distance and find the word that is closer.
* [ ] Define a distance and find the word that furthest.

2. How to classify a text by topic?
* [ ] Use a convolutional neural network to reduce the dimnesionality of the vector space.
* [ ] Use unsupervised learning and k-means clustering.
* [ ] Minimize the distance to the topic's center.
* [ ] Train a logistic regression for each topic.

3. How is the co-ocurrence Matrix built?
* [ ] Count the ajacent words based on an N-sized window. 
* [ ] Use a RegEx to find every unique words.
* [ ] Train a neural network to predict the appearance of a word.
* [ ] Find all the synonyms for each word.

4. What is an incorrect distance metric?
* [ ] The Euclidean Distance.
* [ ] The Absolute Value Distance.
* [ ] The Identity Distance.
* [ ] Cosine Similarity.

5. What is the main innovation about GloVe word embeddings?
* [ ] GloVe dismisses stop words.
* [ ] GloVe uses two different co-ocurrence matrix.
* [ ] GloVe defines the N-sized window based on punctuation signs.
* [ ] GloVe applies a linear transformation to the co-ocurrence matrix.


## Open Questions
1. Propose a potential application for word embeddings?
2. What concept did you found most interesting? Why?
3. Can you imagine a positive alteration as to how the co-ocurrence is build?
