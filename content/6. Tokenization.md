# 6. Tokenization

The first step to train word embeddings is to separate the text by words. This can be done trivially through regex, but there is also a long tradintion in NLP (Natural Language Processing) that precedes Deep Learning and consists of manually aggregating words that have the same root, this includes plurals and conjugated verbs.

Other points to consider are words that ocurr frequently and add little extra context this includes most prepositions and are usually referred as stop words. The other end of the spectrum requires extra attention, as for low frenquency words it may be difficult to get suffiecient context or they maybe product of an ortogrpahic error.

Ideally, the best deep learning models should take care of this anomalies by themselves, but its important to note. That optimizations in this step can produce a reduction of training time, and increase of accuracy. For our purposes we are going to use a simple regex, and do not worry about subtlties [2](https://web.stanford.edu/~jurafsky/slp3/2.pdf):

```
// Tokenization RegEx.
```

To exemplify some of the difficulties we need to deal when using a RegEx to tokenize a corpus, consider the following:

* Abbreviations and compound words like N.Y.C and New York
* Words with internal hyphens
* Numbers with symbols like $, %
* Various punctuation symbols like ellipsis
* Additional tokens like parenthesis.


Now, to start training your own word embeddings, please insert a medium to long text (minimum 1000 words) for tokenization.

![Input for Text Tokenization.]()
